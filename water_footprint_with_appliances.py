# -*- coding: utf-8 -*-
"""water_footprint_with_appliances.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FlreDxiFBJbHgN5h87RfQfHNDPm40Ov6
"""

import pandas as pd

# Load dataset
df = pd.read_csv("water_footprint_dataset_with_appliances.csv")

# 1. Remove duplicates
df = df.drop_duplicates()

# 2. Standardize categorical column
df['Primary_Appliance'] = df['Primary_Appliance'].str.strip().str.title()

# 3. Ensure no negative values in numeric columns
numeric_cols = ['Usage_Hours', 'Family_Size', 'Appliances', 'Water_Saving_Device', 'Water_Usage']
for col in numeric_cols:
    df = df[df[col] >= 0]

# 4. Convert binary column to categorical
df['Water_Saving_Device'] = df['Water_Saving_Device'].map({0: 'No', 1: 'Yes'})

# 5. Reset index after cleaning
df = df.reset_index(drop=True)

# âœ… Save cleaned dataset (overwrite original or create a new file)
df.to_csv("water_footprint_dataset_with_appliances_cleaned.csv", index=False)

# Final cleaned dataset info
print("âœ… Cleaned dataset saved as 'water_footprint_dataset_with_appliances_cleaned.csv'")
print(df.info())
print(df.head())

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Load cleaned dataset
df = pd.read_csv("/content/water_footprint_dataset_with_appliances_cleaned.csv")

# -------------------------------
# Feature Engineering
# -------------------------------
# Per-capita water usage
df["Water_Usage_Per_Capita"] = df["Water_Usage"] / df["Family_Size"]

# Interaction feature: usage hours * appliances
df["Usage_X_Appliances"] = df["Usage_Hours"] * df["Appliances"]

# Encode categorical column
df["Primary_Appliance"] = df["Primary_Appliance"].astype("category").cat.codes
df["Water_Saving_Device"] = df["Water_Saving_Device"].map({"No": 0, "Yes": 1})

# -------------------------------
# Summary Statistics
# -------------------------------
print("\n=== Descriptive Statistics ===")
print(df.describe())

print("\n=== Correlation Matrix ===")
print(df.corr())

# -------------------------------
# Train/Test Split
# -------------------------------
X = df.drop(columns=["Water_Usage"])
y = df["Water_Usage"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# âœ… Print dataset shapes
print("\n=== Dataset Split Info ===")
print(f"Training set: X_train = {X_train.shape}, y_train = {y_train.shape}")
print(f"Test set:     X_test  = {X_test.shape}, y_test  = {y_test.shape}")

# -------------------------------
# Models
# -------------------------------
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42, n_estimators=200),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42, n_estimators=200)
}

results = []

for name, model in models.items():
    # Use scaled data for linear, otherwise original
    if name == "Linear Regression":
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    # Metrics
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)

    # Cross-validation (on training set)
    if name == "Linear Regression":
        cv_r2 = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring="r2").mean()
    else:
        cv_r2 = cross_val_score(model, X_train, y_train, cv=5, scoring="r2").mean()

    results.append([name, r2, rmse, mae, cv_r2])

# -------------------------------
# Save Results
# -------------------------------
results_df = pd.DataFrame(results, columns=["Model", "R2", "RMSE", "MAE", "CV_R2"])
results_df = results_df.sort_values(by="R2", ascending=False).reset_index(drop=True)

print("\n=== Model Comparison ===")
print(results_df)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score

# -------------------------------
# Load Cleaned Dataset
# -------------------------------
df = pd.read_csv("water_footprint_dataset_with_appliances_cleaned.csv")

# Feature engineering
df["Water_Usage_Per_Capita"] = df["Water_Usage"] / df["Family_Size"]
df["Usage_X_Appliances"] = df["Usage_Hours"] * df["Appliances"]

# Encode categorical
df["Primary_Appliance"] = df["Primary_Appliance"].astype("category").cat.codes
df["Water_Saving_Device"] = df["Water_Saving_Device"].map({"No": 0, "Yes": 1})

# Split features and target
X = df.drop(columns=["Water_Usage"])
y = df["Water_Usage"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scale for linear regression
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# -------------------------------
# Train Models
# -------------------------------
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42, n_estimators=200),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42, n_estimators=200)
}

best_model = None
best_r2 = -np.inf

for name, model in models.items():
    if name == "Linear Regression":
        model.fit(X_train_scaled, y_train)
        r2 = r2_score(y_test, model.predict(X_test_scaled))
    else:
        model.fit(X_train, y_train)
        r2 = r2_score(y_test, model.predict(X_test))

    print(f"{name} RÂ² = {r2:.4f}")
    if r2 > best_r2:
        best_model = model
        best_r2 = r2
        best_model_name = name

print(f"\nâœ… Best Model: {best_model_name} (RÂ² = {best_r2:.4f})")

# -------------------------------
# Prediction Function
# -------------------------------
def predict_water_usage(family_size, usage_hours, appliances, primary_appliance, water_saving_device):
    # Encode inputs
    primary_mapping = {cat: code for code, cat in enumerate(df["Primary_Appliance"].astype("category").cat.categories)}
    primary_encoded = primary_mapping.get(primary_appliance.title(), 0)

    device_encoded = 1 if water_saving_device.lower() in ["yes", "y", "1"] else 0

    # Feature engineering
    water_usage_per_capita = 0  # placeholder, model will predict instead
    usage_x_appliances = usage_hours * appliances

    # Create input dataframe
    input_data = pd.DataFrame([[
        usage_hours, family_size, appliances, primary_encoded,
        device_encoded, water_usage_per_capita, usage_x_appliances
    ]], columns=X.columns)

    # Scale if best model is Linear Regression
    if best_model_name == "Linear Regression":
        input_data_scaled = scaler.transform(input_data)
        prediction = best_model.predict(input_data_scaled)[0]
    else:
        prediction = best_model.predict(input_data)[0]

    # Calculate savings if no vs yes device
    if device_encoded == 1:
        # Predict again with device = 0 (no saving device)
        input_data_no_device = input_data.copy()
        input_data_no_device["Water_Saving_Device"] = 0
        if best_model_name == "Linear Regression":
            pred_no_device = best_model.predict(scaler.transform(input_data_no_device))[0]
        else:
            pred_no_device = best_model.predict(input_data_no_device)[0]
        savings = pred_no_device - prediction
    else:
        savings = 0

    return prediction, savings

# -------------------------------
# Example Usage
# -------------------------------
pred, savings = predict_water_usage(
    family_size=4,
    usage_hours=6,
    appliances=3,
    primary_appliance="Sink",
    water_saving_device="Yes"
)

print(f"\nðŸ’§ Predicted Water Usage: {pred:.2f} liters")
print(f"ðŸ’° Estimated Savings with device: {savings:.2f} liters")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from scipy.stats import randint, uniform

# -------------------------------
# Load Cleaned Dataset
# -------------------------------
df = pd.read_csv("water_footprint_dataset_with_appliances_cleaned.csv")

# Feature engineering
df["Water_Usage_Per_Capita"] = df["Water_Usage"] / df["Family_Size"]
df["Usage_X_Appliances"] = df["Usage_Hours"] * df["Appliances"]

# Encode categorical
df["Primary_Appliance"] = df["Primary_Appliance"].astype("category").cat.codes
df["Water_Saving_Device"] = df["Water_Saving_Device"].map({"No": 0, "Yes": 1})

# Split features and target
X = df.drop(columns=["Water_Usage"])
y = df["Water_Usage"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# -------------------------------
# Define Models & Parameter Grids
# -------------------------------
models_params = {
    "Decision Tree": (
        DecisionTreeRegressor(random_state=42),
        {
            "max_depth": randint(2, 20),
            "min_samples_split": randint(2, 20),
            "min_samples_leaf": randint(1, 20)
        }
    ),
    "Random Forest": (
        RandomForestRegressor(random_state=42),
        {
            "n_estimators": randint(100, 500),
            "max_depth": randint(2, 20),
            "min_samples_split": randint(2, 20),
            "min_samples_leaf": randint(1, 20)
        }
    ),
    "Gradient Boosting": (
        GradientBoostingRegressor(random_state=42),
        {
            "n_estimators": randint(100, 500),
            "learning_rate": uniform(0.01, 0.3),
            "max_depth": randint(2, 10),
            "min_samples_split": randint(2, 20),
            "min_samples_leaf": randint(1, 20)
        }
    ),
    "XGBoost": (
        XGBRegressor(random_state=42, objective="reg:squarederror"),
        {
            "n_estimators": randint(100, 500),
            "learning_rate": uniform(0.01, 0.3),
            "max_depth": randint(2, 10),
            "subsample": uniform(0.5, 0.5),
            "colsample_bytree": uniform(0.5, 0.5)
        }
    )
}

# -------------------------------
# Hyperparameter Tuning
# -------------------------------
results = []

for name, (model, param_dist) in models_params.items():
    print(f"\nðŸ”Ž Tuning {name}...")
    search = RandomizedSearchCV(
        model,
        param_distributions=param_dist,
        n_iter=30,
        scoring="r2",
        cv=5,
        random_state=42,
        n_jobs=-1,
        verbose=1
    )
    search.fit(X_train, y_train)

    # Best model
    best_model = search.best_estimator_
    y_pred = best_model.predict(X_test)

    # Metrics
    test_r2 = r2_score(y_test, y_pred)
    cv_r2 = search.best_score_
    best_params = search.best_params_

    results.append([name, best_params, cv_r2, test_r2])

# -------------------------------
# Compare Results
# -------------------------------
results_df = pd.DataFrame(results, columns=["Model", "Best Params", "CV_R2", "Test_R2"])
results_df = results_df.sort_values(by="Test_R2", ascending=False).reset_index(drop=True)

print("\n=== Tuned Model Comparison ===")
print(results_df)

results_df.to_csv("tuned_model_results.csv", index=False)
print("\nâœ… Results saved to 'tuned_model_results.csv'")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import r2_score

# -------------------------------
# Load Cleaned Dataset
# -------------------------------
df = pd.read_csv("water_footprint_dataset_with_appliances_cleaned.csv")

# Feature engineering
df["Water_Usage_Per_Capita"] = df["Water_Usage"] / df["Family_Size"]
df["Usage_X_Appliances"] = df["Usage_Hours"] * df["Appliances"]

# Encode categorical
df["Primary_Appliance"] = df["Primary_Appliance"].astype("category").cat.codes
df["Water_Saving_Device"] = df["Water_Saving_Device"].map({"No": 0, "Yes": 1})

# Split features and target
X = df.drop(columns=["Water_Usage"])
y = df["Water_Usage"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# -------------------------------
# Train Final Tuned Gradient Boosting
# -------------------------------
best_params = {
    "learning_rate": 0.2087566853061946,
    "max_depth": 6,
    "min_samples_leaf": 4,
    "min_samples_split": 12,
    "n_estimators": 430
}

gb_model = GradientBoostingRegressor(random_state=42, **best_params)
gb_model.fit(X_train, y_train)

# Evaluate
y_pred = gb_model.predict(X_test)
print(f"âœ… Final Tuned Gradient Boosting RÂ² on Test Set: {r2_score(y_test, y_pred):.4f}")

# -------------------------------
# Prediction Function
# -------------------------------
def predict_water_usage(family_size, usage_hours, appliances, primary_appliance, water_saving_device):
    # Encode inputs
    primary_mapping = {cat: code for code, cat in enumerate(df["Primary_Appliance"].astype("category").cat.categories)}
    primary_encoded = primary_mapping.get(primary_appliance.title(), 0)

    device_encoded = 1 if water_saving_device.lower() in ["yes", "y", "1"] else 0

    # Feature engineering
    water_usage_per_capita = 0  # model predicts overall usage
    usage_x_appliances = usage_hours * appliances

    # Create input dataframe
    input_data = pd.DataFrame([[
        usage_hours, family_size, appliances, primary_encoded,
        device_encoded, water_usage_per_capita, usage_x_appliances
    ]], columns=X.columns)

    # Predict usage
    prediction = gb_model.predict(input_data)[0]

    # Calculate savings if device = Yes
    if device_encoded == 1:
        input_data_no_device = input_data.copy()
        input_data_no_device["Water_Saving_Device"] = 0
        pred_no_device = gb_model.predict(input_data_no_device)[0]
        savings = pred_no_device - prediction
    else:
        savings = 0

    return prediction, savings

# -------------------------------
# Example Usage
# -------------------------------
# -------------------------------
# Example Usage
# -------------------------------
usage_hours = 5
family_size = 4
# Add placeholder values for appliances and primary_appliance
appliances = 3
primary_appliance = "Sink"


# Case 1: Without water-saving device
pred_no, _ = predict_water_usage(usage_hours=usage_hours, family_size=family_size, appliances=appliances, primary_appliance=primary_appliance, water_saving_device="No")

# Case 2: With water-saving device
pred_yes, _ = predict_water_usage(usage_hours=usage_hours, family_size=family_size, appliances=appliances, primary_appliance=primary_appliance, water_saving_device="Yes")

# Calculate savings
savings = pred_no - pred_yes

print(f"\nðŸ”¹ Scenario Analysis Example (Usage_Hours={usage_hours}, Family_Size={family_size}):")
print(f"Predicted Without Device: {pred_no:.2f} liters")
print(f"Predicted With Device:    {pred_yes:.2f} liters")
print(f"ðŸ’° Predicted Water Saved:  {savings:.2f} liters")

import joblib

# Save trained model and feature columns
joblib.dump(gb_model, "model.pkl")
joblib.dump(X.columns.tolist(), "X_columns.pkl")